{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment lab 03\n",
    "- 29.03.21\n",
    "\n",
    "## Master Class: Machine Learning (5MI2018)\n",
    "- Faculty of Economic Science\n",
    "- University of Neuchatel (Switzerland)\n",
    "- Lab 3, see ML21_Exercise_3.pdf for more information\n",
    "\n",
    "## Authors: \n",
    "- Romain Claret @RomainClaret\n",
    "- Sylvain Robert-Nicoud @Nic0uds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the features names and the values of the categories from adult.names (build a dictionary)\n",
    "\n",
    "data_dict = {}\n",
    "with open('adult.names') as f:\n",
    "    for l in f:\n",
    "        if l[0] == '|' or ':' not in l: continue\n",
    "        c = l.split(':')\n",
    "        if c[1].startswith(' continuous'): data_dict[c[0]] = \"\"\n",
    "        else: data_dict[c[0]] = c[1].replace(\"\\n\",\"\").replace(\".\",\"\").replace(\" \",\"\").split(\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py:767: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  return read_csv(**locals())\n",
      "/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py:767: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  return read_csv(**locals())\n"
     ]
    }
   ],
   "source": [
    "# in the specifications (adult.names): Unknown values are replaced with the character '?'\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "header = list(data_dict.keys())+['income']\n",
    "df_train = pd.read_table(\"adult.data\", sep=r',\\s', na_values='?', header=None, names=header).dropna()\n",
    "df_evaluate = pd.read_table(\"adult.test\", sep=r',\\s', na_values='?', skiprows=[0], header=None, names=header).dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import pandas_profiling\n",
    "#df_train.profile_report()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_evaluate.profile_report()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardizing of numeric values\n",
    "# Doesn't have a lot of meaning in the case of decision trees as it's not using distances (like KNN)\n",
    "# But it's just a pedagological flavor\n",
    "# https://stats.stackexchange.com/questions/10289/whats-the-difference-between-normalization-and-standardization\n",
    "\n",
    "#for c in df_train.select_dtypes(exclude=['object']):\n",
    "#    df_train[\"stand_\"+c] = df_train[c] - (df_train[c].mean() / df_train[c].std())\n",
    "#    \n",
    "#for c in df_evaluate.select_dtypes(exclude=['object']):\n",
    "#    df_evaluate[\"stand_\"+c] = df_evaluate[c] - (df_evaluate[c].mean() / df_evaluate[c].std())\n",
    "#\n",
    "#df_train.info(verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# droping the education because it's redundant with education-num\n",
    "# droping the occupation because it's not generic enough, we have much more categories that those captured in the training sample\n",
    "# droping the relationship because it's not generic enough, we have much more categories that those captured in the training sample\n",
    "\n",
    "drop_list = [\"education\", \"occupation\", \"relationship\"]\n",
    "df_train = df_train.drop(columns=drop_list)\n",
    "df_evaluate = df_evaluate.drop(columns=drop_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reducing categories with multiple options into lower dimensions classification (into binary preferably) when possible\n",
    "# - marital-status could be reduced as Married or Not-Married\n",
    "# marital-status ['Never-married' 'Married-civ-spouse' 'Divorced' 'Married-spouse-absent' 'Separated' 'Married-AF-spouse' 'Widowed']\n",
    "# - workclass could be recuded to 3 dimensions: Government, Private, and Self-Employment\n",
    "# Note that we take into consideration all the options for the category from the specifications\n",
    "# ['State-gov' 'Self-emp-not-inc' 'Private' 'Federal-gov' 'Local-gov' 'Self-emp-inc' 'Without-pay']\n",
    "\n",
    "dict_replace = {\n",
    "    'marital-status' : {\n",
    "        'Never-married': 'Not-Married',\n",
    "        'Married-civ-spouse': 'Married',\n",
    "        'Divorced': 'Not-Married',\n",
    "        'Married-spouse-absent': 'Married',\n",
    "        'Separated': 'Married',\n",
    "        'Married-AF-spouse': 'Married',\n",
    "        'Widowed': 'Not-Married'\n",
    "        },\n",
    "    'workclass': {\n",
    "        'State-gov': 'Government',\n",
    "        'Self-emp-not-inc': 'Self-Employment',\n",
    "        'Federal-gov': 'Government',\n",
    "        'Local-gov': 'Government',\n",
    "        'Self-emp-inc': 'Self-Employment'\n",
    "        }\n",
    "}\n",
    "\n",
    "df_train.replace(dict_replace, inplace=True)\n",
    "df_evaluate.replace(dict_replace, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uniformizing the categories between the training and evaluation datasets\n",
    "# indeed, there is a . at the end of the value in the evaluation dataset for the income category and not in the training dataset\n",
    "df_evaluate[\"income\"].replace({\"<=50K.\": \"<=50K\", \">50K.\": \">50K\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "# for binary categories we will be using a label encoder\n",
    "# - marital-status, sex, income\n",
    "\n",
    "for l in [\"marital-status\", \"sex\", \"income\"]:\n",
    "    l_enc = LabelEncoder()\n",
    "    encoder_train = l_enc.fit(df_train[l])\n",
    "    encoder_evaluate = l_enc.fit(df_evaluate[l])\n",
    "    df_train[\"encoded_\"+l] = encoder_train.transform(df_train[l])\n",
    "    df_evaluate[\"encoded_\"+l] = encoder_evaluate.transform(df_evaluate[l])\n",
    "    \n",
    "#df_train.reset_index(inplace=True,drop=True)\n",
    "#df_evaluate.reset_index(inplace=True,drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For non-binary categories, first we check the specifications of the dataset to validate all the options per category (we have data_dict)\n",
    "# Indeed, the values in the categories are not always all present in a dataset\n",
    "# race: White, Asian-Pac-Islander, Amer-Indian-Eskimo, Other, Black.\n",
    "# native-country: United-States, Cambodia, England, Puerto-Rico, Canada, Germany, Outlying-US(Guam-USVI-etc), India, Japan, Greece, South, China, Cuba, Iran, Honduras, Philippines, Italy, Poland, Jamaica, Vietnam, Mexico, Portugal, Ireland, France, Dominican-Republic, Laos, Ecuador, Taiwan, Haiti, Columbia, Hungary, Guatemala, Nicaragua, Scotland, Thailand, Yugoslavia, El-Salvador, Trinadad&Tobago, Peru, Hong, Holand-Netherlands.\n",
    "# and our custom category: workclass: Government, Private, and Self-Employment\n",
    "# adding temporary fake data for the one hot encoder\n",
    "\n",
    "fake_row = df_train[:1].copy()\n",
    "df_fake = pd.DataFrame(data=fake_row, columns=df_train.columns)\n",
    "\n",
    "cats_nonbinary = [\"race\", \"native-country\"]\n",
    "\n",
    "for c in cats_nonbinary:\n",
    "    for v in data_dict[c]:\n",
    "        fake_row[c] = v\n",
    "        df_fake = df_fake.append(fake_row, ignore_index=True)\n",
    "        \n",
    "cat_workclass = [\"Government\", \"Private\", \"Self-Employment\"]\n",
    "for cw in cat_workclass:\n",
    "    fake_row[\"workclass\"] = cw\n",
    "    df_fake = df_fake.append(fake_row, ignore_index=True)\n",
    "    \n",
    "df_train = df_train.append(df_fake).reset_index(drop=True)\n",
    "df_evaluate = df_evaluate.append(df_fake).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#not_columns = ['encoded_income',\"encoded_sex\", \"encoded_marital-status\", \"income\"]\n",
    "#df_train.loc[:, df_train.columns != 'encoded_income']\n",
    "#df_train[map(lambda x :x not in not_columns, list(df_train.columns))]\n",
    "#df_train[df_train.columns.difference(not_columns)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['age',\n",
       " 'fnlwgt',\n",
       " 'education-num',\n",
       " 'capital-gain',\n",
       " 'capital-loss',\n",
       " 'hours-per-week',\n",
       " 'workclass',\n",
       " 'race',\n",
       " 'native-country',\n",
       " 'encoded_marital-status',\n",
       " 'encoded_sex',\n",
       " 'encoded_income']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get meaningful columns\n",
    "continuous_features = [k for k, v in data_dict.items() if v == \"\"]\n",
    "unencoded_features = [\"workclass\", \"race\", \"native-country\"]\n",
    "encoded_features = [c for c in df_train if c.startswith('encoded')]\n",
    "columns = continuous_features+unencoded_features+encoded_features\n",
    "#columns.remove(\"encoded_income\")\n",
    "columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#encoded_features[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Standardizing of numeric values\n",
    "# Doesn't have a lot of meaning in the case of decision trees as it's not using distances (like KNN)\n",
    "# But it's just a pedagological flavor and to use the ColumnTranformer for whatever reason \n",
    "# https://stats.stackexchange.com/questions/10289/whats-the-difference-between-normalization-and-standardization\n",
    "\n",
    "# We choose a the best parameters from lab2 for the decision tree\n",
    "# depth=8 Train accuracy_score 0.8550292179535\n",
    "# depth=8 Test accuracy_score 0.8465108569534229\n",
    "# depth=8 Evaluation accuracy_score 0.8469455511288181 \n",
    "\n",
    "feature_transformation = ColumnTransformer(transformers=[\n",
    "    ('categorical', OneHotEncoder(handle_unknown='ignore'), unencoded_features+encoded_features[:-1]),\n",
    "    ('numerical', StandardScaler(), continuous_features)\n",
    "])\n",
    "\n",
    "adult_pipeline = Pipeline(steps=[\n",
    "  ('features', feature_transformation),\n",
    "  ('classifier', DecisionTreeClassifier(criterion='gini', random_state=1, max_depth=8))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building our pipeline-driven model\n",
    "pipeline_model = adult_pipeline.fit(df_train, df_train[\"encoded_income\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('features',\n",
       "                 ColumnTransformer(transformers=[('categorical',\n",
       "                                                  OneHotEncoder(handle_unknown='ignore'),\n",
       "                                                  ['workclass', 'race',\n",
       "                                                   'native-country',\n",
       "                                                   'encoded_marital-status',\n",
       "                                                   'encoded_sex']),\n",
       "                                                 ('numerical', StandardScaler(),\n",
       "                                                  ['age', 'fnlwgt',\n",
       "                                                   'education-num',\n",
       "                                                   'capital-gain',\n",
       "                                                   'capital-loss',\n",
       "                                                   'hours-per-week'])])),\n",
       "                ('classifier',\n",
       "                 DecisionTreeClassifier(max_depth=8, random_state=1))])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#y_pred = pipeline_model.predict(df_evaluate)\n",
    "#y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for non-binary categories we will be using a onehot encoder as decision trees are sensitive to leaves values\n",
    "# note that get_dummies from pandas is exactly doing this without the complexity of using OneHotEncoder manually from sklearn\n",
    "# - workclass, race, native-country\n",
    "\n",
    "#for l in [\"workclass\", \"race\", \"native-country\"]:\n",
    "#    df_train=pd.concat([df_train,pd.get_dummies(df_train[l], prefix=\"encoded_\"+l)],axis=1)\n",
    "#    df_evaluate=pd.concat([df_evaluate,pd.get_dummies(df_evaluate[l], prefix=\"encoded_\"+l)],axis=1)\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove the fake rows\n",
    "df_train = df_train[:-len(df_fake)]\n",
    "df_evaluate = df_evaluate[:-len(df_fake)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get meaningful columns\n",
    "#continuous_features = [k for k, v in data_dict.items() if v == \"\"]\n",
    "#encoded_features = [c for c in df_train if c.startswith('encoded')]\n",
    "#columns = continuous_features+encoded_features\n",
    "#columns.remove(\"encoded_income\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Converting objects columns into values\n",
    "#for c in continuous_features+[\"encoded_income\"]:\n",
    "#    df_train[c] = df_train[c].astype('int64')\n",
    "#    df_evaluate[c] = df_evaluate[c].astype('int64')\n",
    "#\n",
    "#for c in [c for c in df_train.select_dtypes(['object']) if c.startswith('encoded')]:\n",
    "#    df_train[c] = df_train[c].astype('uint8')\n",
    "#    df_evaluate[c] = df_evaluate[c].astype('uint8')\n",
    "#\n",
    "#df_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from sklearn.model_selection import train_test_split\n",
    "#\n",
    "## make training and testings sets\n",
    "#X_train, X_test, y_train, y_test = train_test_split(df_train[columns],df_train[\"encoded_income\"],test_size=0.2,random_state=1)\n",
    "#\n",
    "## make evaluation sets\n",
    "#X_evaluate = df_evaluate[columns]\n",
    "#y_evaluate = df_evaluate[\"encoded_income\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# make training and testings sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_train,df_train[\"encoded_income\"],test_size=0.2,random_state=1)\n",
    "\n",
    "# make evaluation sets\n",
    "X_evaluate = df_evaluate\n",
    "y_evaluate = df_evaluate[\"encoded_income\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_evaluate.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run a decision tree classifier\n",
    "\n",
    "#from sklearn.tree import DecisionTreeClassifier\n",
    "#from sklearn.metrics import accuracy_score\n",
    "#import matplotlib.pyplot as plt\n",
    "#\n",
    "#parameter_dtree_min = 1\n",
    "#parameter_dtree_max = 15\n",
    "#preds_dtree_train=[]\n",
    "#preds_dtree_test=[]\n",
    "#for depth in range(parameter_dtree_min,parameter_dtree_max):\n",
    "#    cl_dtree = DecisionTreeClassifier(criterion='gini', random_state=1,max_depth=depth)\n",
    "#    dtree_model = cl_dtree.fit(X_train,y_train)\n",
    "#    y_hat_dtree_train = dtree_model.predict(X_train)\n",
    "#    y_hat_dtree_test = dtree_model.predict(X_test)\n",
    "#    preds_dtree_train.append(accuracy_score(y_train,y_hat_dtree_train))\n",
    "#    preds_dtree_test.append(accuracy_score(y_test,y_hat_dtree_test))\n",
    "#    #print(depth,\"Train accuracy_score\",preds_train[-1])\n",
    "#    #print(depth,\"Test accuracy_score\",preds_test[-1],\"\\n\")\n",
    "#    \n",
    "#plt.scatter(range(parameter_dtree_min,parameter_dtree_max),preds_dtree_train,c=\"b\",label=\"train score\")\n",
    "#plt.scatter(range(parameter_dtree_min,parameter_dtree_max),preds_dtree_test,c=\"r\",label=\"test score\")\n",
    "#plt.legend(loc=\"upper left\")\n",
    "#plt.title('DecisionTreeClassifier: accuracy_score vs depth')\n",
    "#plt.xlabel('depth')\n",
    "#plt.ylabel('accuracy_score')\n",
    "#plt.savefig('DecisionTreeClassifier.png')\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy_score 0.8539931203116582\n",
      "Test accuracy_score 0.8518150174042765\n",
      "Evaluation accuracy_score 0.8470783532536521 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "#present depth with best score for evaluation dataset\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "y_hat_dtree_train = pipeline_model.predict(X_train)\n",
    "y_hat_dtree_test = pipeline_model.predict(X_test)\n",
    "y_hat_dtree_evaluate = pipeline_model.predict(X_evaluate)\n",
    "\n",
    "print(\"Train accuracy_score\",accuracy_score(y_train,y_hat_dtree_train))\n",
    "print(\"Test accuracy_score\",accuracy_score(y_test,y_hat_dtree_test))\n",
    "print(\"Evaluation accuracy_score\",accuracy_score(y_evaluate,y_hat_dtree_evaluate),\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "pickle.dump(pipeline_model, open(\"pipeline_model.pickle\", \"wb\" ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'adult.test'\n",
    "file_handler = open(filename, 'r').readlines()\n",
    "prefix_file = \"adult_2021_cw_\"\n",
    "week_number = 1\n",
    "split_into = 10\n",
    "line_count = 0\n",
    "with open(filename) as f: line_count = sum(1 for _ in f)\n",
    "\n",
    "for i in range(len(file_handler)):\n",
    "    if i % (line_count//split_into) == 0:\n",
    "        open(str(prefix_file)+str(week_number) + \".csv\", \"w+\").writelines(file_handler[i:i+1000])\n",
    "        week_number += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "pipeline_model = pickle.load( open(\"pipeline_model.pickle\", \"rb\" ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adult_2021_cw_1.csv accuracy_score: 0.8293736501079914 \n",
      "\n",
      "adult_2021_cw_2.csv accuracy_score: 0.8503253796095445 \n",
      "\n",
      "adult_2021_cw_3.csv accuracy_score: 0.8427807486631016 \n",
      "\n",
      "adult_2021_cw_4.csv accuracy_score: 0.8307860262008734 \n",
      "\n",
      "adult_2021_cw_5.csv accuracy_score: 0.8507462686567164 \n",
      "\n",
      "adult_2021_cw_6.csv accuracy_score: 0.854978354978355 \n",
      "\n",
      "adult_2021_cw_7.csv accuracy_score: 0.8545454545454545 \n",
      "\n",
      "adult_2021_cw_8.csv accuracy_score: 0.8514531754574811 \n",
      "\n",
      "adult_2021_cw_9.csv accuracy_score: 0.8296943231441049 \n",
      "\n",
      "adult_2021_cw_10.csv accuracy_score: 0.8574537540805223 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "weeks_count = 10\n",
    "filename = 'adult.test'\n",
    "prefix_file = \"adult_2021_cw_\"\n",
    "\n",
    "# get the features names and the values of the categories from adult.names (build a dictionary)\n",
    "\n",
    "data_dict = {}\n",
    "with open('adult.names') as f:\n",
    "    for l in f:\n",
    "        if l[0] == '|' or ':' not in l: continue\n",
    "        c = l.split(':')\n",
    "        if c[1].startswith(' continuous'): data_dict[c[0]] = \"\"\n",
    "        else: data_dict[c[0]] = c[1].replace(\"\\n\",\"\").replace(\".\",\"\").replace(\" \",\"\").split(\",\")\n",
    "            \n",
    "header = list(data_dict.keys())+['income']\n",
    "            \n",
    "for i in range (weeks_count):\n",
    "    filename = str(prefix_file)+str(i+1)+\".csv\"\n",
    "    df_weekly = pd.read_table(filename, sep=r',\\s', na_values='?', skiprows=[0], header=None, names=header).dropna()\n",
    "    \n",
    "    drop_list = [\"education\", \"occupation\", \"relationship\"]\n",
    "    df_weekly = df_weekly.drop(columns=drop_list)\n",
    "    \n",
    "    dict_replace = {\n",
    "    'marital-status' : {\n",
    "        'Never-married': 'Not-Married',\n",
    "        'Married-civ-spouse': 'Married',\n",
    "        'Divorced': 'Not-Married',\n",
    "        'Married-spouse-absent': 'Married',\n",
    "        'Separated': 'Married',\n",
    "        'Married-AF-spouse': 'Married',\n",
    "        'Widowed': 'Not-Married'\n",
    "        },\n",
    "    'workclass': {\n",
    "        'State-gov': 'Government',\n",
    "        'Self-emp-not-inc': 'Self-Employment',\n",
    "        'Federal-gov': 'Government',\n",
    "        'Local-gov': 'Government',\n",
    "        'Self-emp-inc': 'Self-Employment'\n",
    "        }\n",
    "    }\n",
    "\n",
    "    df_weekly.replace(dict_replace, inplace=True)\n",
    "    \n",
    "    df_weekly[\"income\"].replace({\"<=50K.\": \"<=50K\", \">50K.\": \">50K\"}, inplace=True)\n",
    "    \n",
    "    for l in [\"marital-status\", \"sex\", \"income\"]:\n",
    "        l_enc = LabelEncoder()\n",
    "        encoder_weekly = l_enc.fit(df_weekly[l])\n",
    "        df_weekly[\"encoded_\"+l] = encoder_weekly.transform(df_weekly[l])\n",
    "    \n",
    "    y_hat_dtree_weekly = pipeline_model.predict(df_weekly)\n",
    "\n",
    "    print(filename, \"accuracy_score:\",accuracy_score(df_weekly[\"encoded_income\"],y_hat_dtree_weekly),\"\\n\")\n",
    "    \n",
    "    pd.DataFrame(y_hat_dtree_weekly).to_csv(str(prefix_file)+str(i+1)+\"_pred.csv\",header=[\"pred_income\"], index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lab 01 results:\n",
    "# depth=8 Train accuracy_score 0.8356749140038957\n",
    "# depth=8 Test accuracy_score 0.832587435769932\n",
    "# depth=8 Evaluation accuracy_score 0.8287516600265604 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py:767: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  return read_csv(**locals())\n",
      "/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py:767: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  return read_csv(**locals())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******************************************************\n",
      "Evaluate Decision Tree Classifier Pipeline on new data\n",
      "****************************************************** \n",
      "\n",
      "Train accuracy_score 0.8539931203116582\n",
      "Test accuracy_score 0.8518150174042765\n",
      "Evaluation accuracy_score 0.8470783532536521 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "%run main.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adult_2021_cw_1_pred.csv accuracy_score: 0.8293736501079914 \n",
      "\n",
      "adult_2021_cw_2_pred.csv accuracy_score: 0.8503253796095445 \n",
      "\n",
      "adult_2021_cw_3_pred.csv accuracy_score: 0.8427807486631016 \n",
      "\n",
      "adult_2021_cw_4_pred.csv accuracy_score: 0.8307860262008734 \n",
      "\n",
      "adult_2021_cw_5_pred.csv accuracy_score: 0.8507462686567164 \n",
      "\n",
      "adult_2021_cw_6_pred.csv accuracy_score: 0.854978354978355 \n",
      "\n",
      "adult_2021_cw_7_pred.csv accuracy_score: 0.8545454545454545 \n",
      "\n",
      "adult_2021_cw_8_pred.csv accuracy_score: 0.8514531754574811 \n",
      "\n",
      "adult_2021_cw_9_pred.csv accuracy_score: 0.8296943231441049 \n",
      "\n",
      "adult_2021_cw_10_pred.csv accuracy_score: 0.8574537540805223 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "%run predict_income.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
